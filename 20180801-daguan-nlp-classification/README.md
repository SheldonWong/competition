## 达观杯文本分类竞赛

### 竞赛官网
[“达观杯”文本智能处理挑战赛](http://www.dcjingsai.com/common/cmpt/%E2%80%9C%E8%BE%BE%E8%A7%82%E6%9D%AF%E2%80%9D%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B_%E7%AB%9E%E8%B5%9B%E4%BF%A1%E6%81%AF.html)

- 基本流程：
1. 定义目标
2. 认识数据
3. 模型验证（单机器学习模型、深度学习模型、集成学习）
4. 模型集成


### 数据

- 数据下载地址：

### 数据探索

- 类别分布

### 特征

- 这一步主要是生成sample的表示
- 一般常见数据分三种类型：
	- 结构化数据
	- 文本数据
	- 图像数据

- 这个任务是文本数据，就简单说一下文本的表示
	- 词的表示：one-hot，word2vec等
	- 文章的表示：
		- 高维稀疏表示 - BOW（TF，TFIDF）
		- 低维表示 - BOW的衍生模型生成的表示LSA、LDA、NMF等，基于高维BOW特征，生成Topic向量
		- 深度表示 - word2vec矩阵等

- 为什么要保存特征？
- 特征的提取太耗费时间
- 原始特征与加工的特征？
- 原始特征维度太高，可以通过降维得到低维度的信息

### 模型验证

- 候选模型：KNN、NB、LR、SVM、AdaBoost、GBDT、RF、DeepLearning等

- 高维稀疏： LR、SVM
- 低维稠密： GBDT-XGB-LightGBM
- 深度模型： TextCNN、fasttext

- 工具：gensim、sklearn、keras

- 通常来说，参赛者都会把所有的模型都跑一遍，特征组合+模型组合
- 然后精心调参
- 最后就是组合模型

- 如何调参？
- kfold+grid—search
- 为什么要保存模型？
- 某些模型训练时间太久，如果最终集成的时候要用到模型，再训练一遍太浪费时间。

### 结果存储

- 结果存储：特征-模型-指标.csv，例如bow-lr-0.779.csv

### 结果分析
- 多分类的模型要用

### badcase分析
- 抽取出预测错误的样本，对比

### 改进
- 改进思路1：调整特征，构建新特征，组合特征，数据增强等方法
- 改进思路2：调整模型参数
- 改进思路3：组合模型，基于投票的，基于概率的，soft-voting
- 改进思路4：通常性能是限制调参的一个大问题，如果模型跑的够快，那么调起参来会很方便。分布式，并行，GPU编程了解哈，加速计算了解哈。

